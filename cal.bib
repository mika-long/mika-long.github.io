@online{abernethyBlackwellApproachabilityLowRegret2010,
  title = {Blackwell {{Approachability}} and {{Low-Regret Learning}} Are {{Equivalent}}},
  author = {Abernethy, Jacob and Bartlett, Peter L. and Hazan, Elad},
  date = {2010-11-08},
  eprint = {1011.1936},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1011.1936},
  url = {http://arxiv.org/abs/1011.1936},
  urldate = {2023-05-02},
  abstract = {We consider the celebrated Blackwell Approachability Theorem for two-player games with vector payoffs. We show that Blackwell's result is equivalent, via efficient reductions, to the existence of "no-regret" algorithms for Online Linear Optimization. Indeed, we show that any algorithm for one such problem can be efficiently converted into an algorithm for the other. We provide a useful application of this reduction: the first efficient algorithm for calibrated forecasting.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {C\:\\Users\\slong\\Zotero\\storage\\4IAS64Q7\\Abernethy et al_2010_Blackwell Approachability and Low-Regret Learning are Equivalent.pdf;C\:\\Users\\slong\\Zotero\\storage\\6DTR85HF\\1011.html}
}

@article{bayirGenieOpenBox2018,
  title = {Genie: {{An Open Box Counterfactual Policy Estimator}} for {{Optimizing Sponsored Search Marketplace}}},
  author = {Bayir, Murat Ali and Xu, Mingsen and Zhu, Yaojia and Shi, Yifan},
  date = {2018-08},
  abstract = {In this paper, we propose an offline counterfactual policy estimation framework called Genie to optimize Sponsored Search Marketplace. Genie employs an open box simulation engine with click calibration model to compute the KPI impact of any modification to the system. From the experimental results on Bing traffic, we showed that Genie performs better than existing observational approaches that employs randomized experiments for traffic slices that have frequent policy updates. We also show that Genie can be used to tune completely new policies efficiently without creating risky randomized experiments due to cold start problem. As time of today, Genie hosts more than 10000 optimization jobs yearly which runs more than 30 Million processing node hours of big data jobs for Bing Ads. For the last 3 years, Genie has been proven to be the one of the major platforms to optimize Bing Ads Marketplace due to its reliability under frequent policy changes and its efficiency to minimize risks in real experiments.}
}

@article{corbett-daviesMeasureMismeasureFairness2018,
  title = {The {{Measure}} and {{Mismeasure}} of {{Fairness}}: {{A Critical Review}} of {{Fair Machine Learning}}},
  author = {Corbett-Davies, Sam and Goel, Sharad},
  date = {2018-07},
  abstract = {The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes—like race, gender, and their proxies—are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
  keywords = {Fairness}
}

@article{fischhoffJudgmentDecisionMaking2020,
  title = {Judgment and {{Decision Making}}},
  author = {Fischhoff, Baruch and Broomell, Stephen B.},
  date = {2020-01-04},
  journaltitle = {Annual Review of Psychology},
  shortjournal = {Annu. Rev. Psychol.},
  volume = {71},
  number = {1},
  pages = {331--355},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-010419-050747},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-010419-050747},
  urldate = {2023-02-23},
  abstract = {The science of judgment and decision making involves three interrelated forms of research: analysis of the decisions people face, description of their natural responses, and interventions meant to help them do better. After briefly introducing the field's intellectual foundations, we review recent basic research into the three core elements of decision making: judgment, or how people predict the outcomes that will follow possible choices; preference, or how people weigh those outcomes; and choice, or how people combine judgments and preferences to reach a decision. We then review research into two potential sources of behavioral heterogeneity: individual differences in decision-making competence and developmental changes across the life span. Next, we illustrate applications intended to improve individual and organizational decision making in health, public policy, intelligence analysis, and risk management. We emphasize the potential value of coupling analytical and behavioral research and having basic and applied research inform one another.},
  langid = {english},
  keywords = {/keep\_reading,⭐},
  file = {C\:\\Users\\slong\\Zotero\\storage\\UP7MG54K\\Fischhoff and Broomell - 2020 - Judgment and Decision Making.pdf}
}

@article{fosterAsymptoticCalibration1998a,
  title = {Asymptotic {{Calibration}}},
  author = {Foster, Dean P. and Vohra, Rakesh V.},
  date = {1998},
  journaltitle = {Biometrika},
  volume = {85},
  number = {2},
  eprint = {2337364},
  eprinttype = {jstor},
  pages = {379--390},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  url = {https://www.jstor.org/stable/2337364},
  urldate = {2023-05-02},
  abstract = {Can we forecast the probability of an arbitrary sequence of events happening so that the stated probability of an event happening is close to its empirical probability? We can view this prediction problem as a game played against Nature, where at the beginning of the game Nature picks a data sequence and the forecaster picks a forecasting algorithm. If the forecaster is not allowed to randomise, then Nature wins; there will always be data for which the forecaster does poorly. This paper shows that, if the forecaster can randomise, the forecaster wins in the sense that the forecasted probabilities and the empirical probabilities can be made arbitrarily close to each other.},
  keywords = {/unread},
  file = {C\:\\Users\\slong\\Zotero\\storage\\NZBEZTNM\\Foster_Vohra_1998_Asymptotic Calibration.pdf}
}

@article{fosterCalibratedLearningCorrelated1997,
  title = {Calibrated {{Learning}} and {{Correlated Equilibrium}}},
  author = {Foster, Dean P and Vohra, Rakesh V},
  date = {1997-10},
  journaltitle = {Games and economic behavior},
  volume = {21},
  number = {1},
  pages = {40--55},
  issn = {0899-8256},
  doi = {10.1006/game.1997.0595},
  abstract = {Suppose two players repeatedly meet each other to play a game where 1.each uses a learning rule with the property that it is a calibrated forecast of the other's plays, and 2.each plays a myopic best response to this forecast distribution. Then, the limit points of the sequence of plays are correlated equilibria. In fact, for each correlated equilibrium there is some calibrated learning rule that the players can use which results in their playing this correlated equilibrium in the limit. Thus, the statistical concept of a calibration is strongly related to the game theoretic concept of correlated equilibrium.Journal of Economic LiteratureClassification Numbers: C72,D83,C44.},
  file = {C\:\\Users\\slong\\Zotero\\storage\\74K9EUZF\\Foster_Vohra_1997_Calibrated Learning and Correlated Equilibrium.pdf}
}

@inproceedings{hebert-johnsonMulticalibrationCalibrationComputationallyIdentifiable2018,
  title = {Multicalibration: {{Calibration}} for the ({{Computationally-Identifiable}}) {{Masses}}},
  shorttitle = {Multicalibration},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Hebert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
  date = {2018-07-03},
  pages = {1939--1948},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/hebert-johnson18a.html},
  urldate = {2023-02-19},
  abstract = {We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations. The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group. We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {/unread},
  file = {C\:\\Users\\slong\\Zotero\\storage\\JICSRVKJ\\Hebert-Johnson et al. - 2018 - Multicalibration Calibration for the (Computation.pdf;C\:\\Users\\slong\\Zotero\\storage\\KGTESI2H\\Hebert-Johnson et al. - 2018 - Multicalibration Calibration for the (Computation.pdf}
}

@misc{hernandez-oralloUnifiedViewPerformance,
  title = {A Unified View of Performance Metrics: {{Translating}} Threshold Choice into Expected Classification Loss},
  author = {Hernández-Orallo, José and Flach, Peter},
  url = {https://www.jmlr.org/papers/volume13/hernandez-orallo12a/hernandez-orallo12a.pdf},
  abstract = {Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method.}
}

@inproceedings{kleinbergInherentTradeOffsAlgorithmic2018,
  title = {Inherent {{Trade-Offs}} in {{Algorithmic Fairness}}},
  booktitle = {Abstracts of the 2018 {{ACM International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Kleinberg, Jon},
  date = {2018-06},
  series = {{{SIGMETRICS}} '18},
  pages = {40},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3219617.3219634},
  abstract = {Recent discussion in both the academic literature and the public sphere about classification by algorithms has involved tension between competing notions of what it means for such a classification to be fair to different groups. We consider several of the key fairness conditions that lie at the heart of these debates, and discuss recent research establishing inherent trade-offs between these conditions. We also consider a variety of methods for promoting fairness and related notions for classification and selection problems that involve sets rather than just individuals.},
  isbn = {978-1-4503-5846-0},
  venue = {Irvine, CA, USA},
  keywords = {algorithmic fairness,calibration}
}

@article{nosekPreregistrationHardWorthwhile2019,
  title = {Preregistration {{Is Hard}}, {{And Worthwhile}}},
  author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and family=Veer, given=Anna E., prefix=van ’t, useprefix=true and Vazire, Simine},
  date = {2019-10-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {10},
  pages = {815--818},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.07.009},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661319301846},
  urldate = {2023-02-24},
  abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
  langid = {english},
  keywords = {confirmatory research,exploratory research,preregistration,reproducibility,transparency},
  file = {C\:\\Users\\slong\\Zotero\\storage\\2WBSVKF4\\Nosek et al_2019_Preregistration Is Hard, And Worthwhile.pdf}
}

@misc{okoroaforCalibratingOnlinePredictor,
  title = {Calibrating an Online Predictor via {{Approachability}}},
  author = {Okoroafor, Princewill and Sun, Wen and Kleinberg, Robert},
  url = {https://pokoroafor.github.io/assets/pdf/recalibration.pdf},
  abstract = {Predictive models in ML need to be trustworthy and reliable, which often at the very least means outputting calibrated probabilities. This can be particularly difficult to guarantee in the online prediction setting when the outcome sequence can be generated adversarially. In this paper we introduce a technique using Blackwell's approachability theorem for taking an online predictive model which might not be calibrated and transforming its predictions to calibrated predictions without much increase to the loss of the original model. Our proposed algorithm achieves calibration and accuracy at a faster rate than existing techniques [Kuleshov and Ermon, 2017] and is the first algorithm to offer a flexible tradeoff between calibration error and accuracy in the online setting. We demonstrate this by characterizing the space of jointly achievable calibration and regret using our technique. * Use footnote for providing further information about author (webpage, alternative address)-not for acknowledging funding agencies. Preprint. Under review.}
}

@online{okoroaforNonStochasticCDFEstimation2023,
  title = {Non-{{Stochastic CDF Estimation Using Threshold Queries}}},
  author = {Okoroafor, Princewill and Gupta, Vaishnavi and Kleinberg, Robert and Goh, Eleanor},
  date = {2023-01-13},
  eprint = {2301.05682},
  eprinttype = {arxiv},
  eprintclass = {cs, econ},
  doi = {10.48550/arXiv.2301.05682},
  url = {http://arxiv.org/abs/2301.05682},
  urldate = {2023-05-09},
  abstract = {Estimating the empirical distribution of a scalar-valued data set is a basic and fundamental task. In this paper, we tackle the problem of estimating an empirical distribution in a setting with two challenging features. First, the algorithm does not directly observe the data; instead, it only asks a limited number of threshold queries about each sample. Second, the data are not assumed to be independent and identically distributed; instead, we allow for an arbitrary process generating the samples, including an adaptive adversary. These considerations are relevant, for example, when modeling a seller experimenting with posted prices to estimate the distribution of consumers' willingness to pay for a product: offering a price and observing a consumer's purchase decision is equivalent to asking a single threshold query about their value, and the distribution of consumers' values may be non-stationary over time, as early adopters may differ markedly from late adopters. Our main result quantifies, to within a constant factor, the sample complexity of estimating the empirical CDF of a sequence of elements of \$[n]\$, up to \$\textbackslash varepsilon\$ additive error, using one threshold query per sample. The complexity depends only logarithmically on \$n\$, and our result can be interpreted as extending the existing logarithmic-complexity results for noisy binary search to the more challenging setting where noise is non-stochastic. Along the way to designing our algorithm, we consider a more general model in which the algorithm is allowed to make a limited number of simultaneous threshold queries on each sample. We solve this problem using Blackwell's Approachability Theorem and the exponential weights method. As a side result of independent interest, we characterize the minimum number of simultaneous threshold queries required by deterministic CDF estimation algorithms.},
  pubstate = {preprint},
  keywords = {/keep\_reading,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Economics - Econometrics},
  file = {C\:\\Users\\slong\\Zotero\\storage\\SW3KWV54\\Okoroafor et al_2023_Non-Stochastic CDF Estimation Using Threshold Queries.pdf;C\:\\Users\\slong\\Zotero\\storage\\3VH5THJ7\\2301.html}
}

@online{perchetApproachabilityRegretCalibration2013,
  title = {Approachability, {{Regret}} and {{Calibration}}; Implications and Equivalences},
  author = {Perchet, Vianney},
  date = {2013-01-12},
  eprint = {1301.2663},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1301.2663},
  urldate = {2023-05-09},
  abstract = {Blackwell approachability, regret minimization and calibration are three criteria evaluating a strategy (or an algorithm) in different sequential decision problems, or repeated games between a player and Nature. Although they have at first sight nothing in common, links between have been discovered: both consistent and calibrated strategies can be constructed by following, in some auxiliary game, an approachability strategy. We gathered famous or recent results and provide new ones in order to develop and generalize Blackwell's elegant theory. The final goal is to show how it can be used as a basic powerful tool to exhibit a new class of intuitive algorithms, based on simple geometric properties. In order to be complete, we also prove that approachability can be seen as a byproduct of the very existence of consistent or calibrated strategies.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computer Science and Game Theory},
  file = {C\:\\Users\\slong\\Zotero\\storage\\H7PQPGI9\\Perchet_2013_Approachability, Regret and Calibration\; implications and equivalences.pdf;C\:\\Users\\slong\\Zotero\\storage\\T77TSBYP\\1301.html}
}

@article{perchetExponentialWeightApproachability2015,
  title = {Exponential {{Weight Approachability}}, {{Applications}} to {{Calibration}} and {{Regret Minimization}}},
  author = {Perchet, Vianney},
  date = {2015-03-01},
  journaltitle = {Dynamic Games and Applications},
  shortjournal = {Dyn Games Appl},
  volume = {5},
  number = {1},
  pages = {136--153},
  issn = {2153-0793},
  doi = {10.1007/s13235-014-0119-x},
  url = {https://doi.org/10.1007/s13235-014-0119-x},
  urldate = {2023-06-20},
  abstract = {Basic ideas behind the “exponential weight algorithm” (designed for aggregation or minimization of regret) can be transposed into the theory of Blackwell approachability. Using them, we develop an algorithm—that we call “exponential weight approachability”—bounding the distance of average vector payoffs to some product set, with a logarithmic dependency in the dimension of the ambient space. The classic strategy of Blackwell would get instead a polynomial dependency. This result has important consequences, in several frameworks that emerged both in game theory and machine learning. The most striking application is the construction of algorithms that are calibrated with respect to the family of all balls (we treat in details the case of the uniform norm), with dimension independent and optimal, up to logarithmic factors, rates of convergence. Calibration can also be achieved with respect to all Borel sets, covering and improving the previously known results. Exponential weight approachability can also be used to design an optimal and natural algorithm that minimizes refined notions of regret.},
  langid = {english},
  keywords = {/unread,Approachability,Calibration,Exponential weights,Regret,Uncertainty},
  file = {C\:\\Users\\slong\\Zotero\\storage\\5XT7KJ8V\\Perchet - 2015 - Exponential Weight Approachability, Applications t.pdf}
}

@online{pleissFairnessCalibration2017,
  title = {On {{Fairness}} and {{Calibration}}},
  author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q.},
  date = {2017-11-03},
  eprint = {1709.02012},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1709.02012},
  url = {http://arxiv.org/abs/1709.02012},
  urldate = {2023-02-27},
  abstract = {The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be "fair." In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\slong\\Zotero\\storage\\3XGYSI5M\\Pleiss et al_2017_On Fairness and Calibration.pdf;C\:\\Users\\slong\\Zotero\\storage\\F3JSXZBA\\1709.html}
}

@book{smithUncertaintyQuantificationTheory2013,
  title = {Uncertainty {{Quantification}}: {{Theory}}, {{Implementation}}, and {{Applications}}},
  author = {Smith, Ralph C},
  date = {2013-12},
  publisher = {{SIAM}},
  abstract = {The field of uncertainty quantification is evolving rapidly because of increasing emphasis on models that require quantified uncertainties for large-scale applications, novel algorithm development, and new computational architectures that facilitate implementation of these algorithms. Uncertainty Quantification: Theory, Implementation, and Applications provides readers with the basic concepts, theory, and algorithms necessary to quantify input and response uncertainties for simulation models arising in a broad range of disciplines. The book begins with a detailed discussion of applications where uncertainty quantification is critical for both scientific understanding and policy. It then covers concepts from probability and statistics, parameter selection techniques, frequentist and Bayesian model calibration, propagation of uncertainties, quantification of model discrepancy, surrogate model construction, and local and global sensitivity analysis. The author maintains a complementary web page where readers can find data used in the exercises and other supplementary material.},
  isbn = {978-1-61197-321-1},
  langid = {english}
}

@article{tangEffectsVisualizationInteractivity2014,
  title = {The {{Effects}} of {{Visualization}} and {{Interactivity}} on {{Calibration}} in {{Financial Decision-Making}}},
  author = {Tang, Fengchun and Hess, Traci J. and Valacich, Joseph S. and Sweeney, John T.},
  date = {2014-04-01},
  journaltitle = {Behavioral Research in Accounting},
  shortjournal = {Behavioral Research in Accounting},
  volume = {26},
  number = {1},
  pages = {25--58},
  issn = {1050-4753},
  doi = {10.2308/bria-50589},
  url = {https://doi.org/10.2308/bria-50589},
  urldate = {2023-03-15},
  abstract = {With the increased use of XBRL, financial data are readily available in a universal format, enabling users to dynamically render data with a variety of visual, interactive representations. However, the impact of these interactive visual representations on financial decision-making has received little attention. Further, decision-making research suggests that the presentation of a task (i.e., presentation format) can influence decision-making outcomes such as accuracy, confidence, and the calibration between accuracy and confidence. This study examines how visualization and interactivity affect accuracy, confidence, and calibration in a financial decision-making context. Decision-makers are typically overconfident, and this research proposes that visualization and interactivity provide more informational cues, which can actually further increase overconfidence and reduce calibration in some contexts. An experiment conducted with 157 participants supports the prediction that visualization and interactivity features can increase decision-maker overconfidence. However, interactive visualization, when both interface features are present, increases confidence while also increasing accuracy. As a result, when interactivity and visualization are offered individually, decision-makers are less calibrated, but when both features are offered, decision-makers are more calibrated. Implications for users and designers of interactive visualizations with financial data are discussed.Data Availability: Data are available from the authors upon request.},
  keywords = {/unread},
  file = {C\:\\Users\\slong\\Zotero\\storage\\MU8K4SRX\\The-Effects-of-Visualization-and-Interactivity-on.html}
}

@inproceedings{wrightMechanicalTAPartially2015,
  title = {Mechanical {{TA}}: {{Partially Automated High-Stakes Peer Grading}}},
  booktitle = {Proceedings of the 46th {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author = {Wright, James R and Thornton, Chris and Leyton-Brown, Kevin},
  date = {2015-02},
  series = {{{SIGCSE}} '15},
  pages = {96--101},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2676723.2677278},
  abstract = {We describe Mechanical TA, an automated peer review system, and report on our experience using it over three years. Mechanical TA differs from many other peer review systems by involving human teaching assistants (TAs) as a way to assure review quality. Human TAs both evaluate the peer reviews of students who have not yet demonstrated reviewing proficiency and spot check the reviews of students who have. Mechanical TA also features “calibration” reviews, allowing students to quickly gain experience with the peer-review process. We used Mechanical TA for weekly essay assignments in a class of about 70 students, a course design that would have been impossible if every assignment had had to be graded by a TA. We show evidence that it helped to support student learning, leading us to believe that the system may also be useful to others.},
  isbn = {978-1-4503-2966-8},
  venue = {Kansas City, Missouri, USA},
  keywords = {calibration,computer science education,Jason\_PeerGrading,peer grading,peer review,scalability}
}
